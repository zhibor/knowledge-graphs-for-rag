{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Minimum Viable Graph (MVG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll create the Minimumm Viable Graph consisting of `Chunk` nodes arranged into linked lists.\n",
    "\n",
    "\n",
    "1. Extract text from Form10k files, split into chunks, create `Chunk` nodes\n",
    "2. Enhance each `Chunk` node with a text embedding\n",
    "3. Expand the `Chunk` nodes with `NEXT` relationships to form linked lists\n",
    "\n",
    "```cypher\n",
    "(:Chunk \n",
    "  chunkId: string\n",
    "  cik: int\n",
    "  cusip6: string\n",
    "  item: string\n",
    "  source: string\n",
    "  text: string\n",
    "  textEmbedding: float[]\n",
    ")\n",
    "```\n",
    "\n",
    "```cypher\n",
    "(:Chunk)-[:NEXT]->(:Chunk)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import some python packages, set up global constants, and create a connection to the Neo4j database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'shared.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a GraphDatabase interface\n",
    "\n",
    "You will use the Neo4j `GraphDatabase` interface to send queries to the Neo4j database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect `gdb` to be defined in the shared notebook\n",
    "# gdb = GraphDatabase.driver(uri=NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "result = gdb.execute_query(\"RETURN 'Hello, World!' AS message\")\n",
    "\n",
    "result.records[0].get('message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form 10k pre-preprocessing\n",
    "\n",
    "The Form10k data you will be working with has been preprocessed from the original source. \n",
    "\n",
    "Please see the [Form10k Preprocessing](https://github.com/neo4j-product-examples/data-prep-sec-edgar/) repository for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step inspection of a single form 10k document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with one file\n",
    "\n",
    "Get the the file name and then loading the json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_file_name = [f\"{DATA_DIR}/form10k/\" + x for x in os.listdir(f\"{DATA_DIR}/form10k/\")][0]\n",
    "\n",
    "first_file_as_object = json.load(open(first_file_name))\n",
    "\n",
    "first_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the available keys\n",
    "\n",
    "You can loop through they keys to check what the json contains.\n",
    "\n",
    "The first few keys, with names like \"item1\" are the text extracted from the form 10k.\n",
    "The names match the sections within the form.\n",
    "\n",
    "Then there are some fields that were pulled from the mapping file,\n",
    "including cik, cusip6, cusip and names.\n",
    "\n",
    "The source field contains a URL to the download page on the SEC website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in first_file_as_object.items():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at one of the items\n",
    "\n",
    "Take a look at one of the items to see what the text is like.\n",
    "This full text will be split into chunks and stored in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item1_text = first_file_as_object['item1']\n",
    "\n",
    "item1_text[0:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitter from Langchain\n",
    "\n",
    "You can use a text splitter function from Langchain.\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` will use newlines\n",
    "and then whitespace characters to break down a text until\n",
    "the chunks are small enough. This strategy is generally\n",
    "good at keeping paragraphs together.\n",
    "\n",
    "Set a chunk size of 2000 characters,\n",
    "with 200 characters of overlap between each chunk,\n",
    "using the built-in `len` function to calculate the \n",
    "text length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting text into chunks using the RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitter demonstration\n",
    "\n",
    "You can see what the text splitter will do by splitting up\n",
    "the `item1_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item1_text_chunks = text_splitter.split_text(item1_text)\n",
    "item1_text_chunks[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for loading form 10k data\n",
    "\n",
    "You can create a helper function to load the form 10k data from the json files.\n",
    "\n",
    "This function will load the file as a json object, \n",
    "then for each text section in the json, it will split the text into chunks.\n",
    "For each chunk, it will create a new object with the chunk text and the metadata from the form 10k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def split_form10k_data_from_file(file):\n",
    "    chunks_with_metadata = [] # use this to accumlate chunk records\n",
    "    file_as_object = json.load(open(file)) # open the json file\n",
    "    for item in ['item1','item1a','item7','item7a']: # pull these keys from the json\n",
    "        print(f'Processing {item} from {file}') \n",
    "        item_text = file_as_object[item] # grab the text of the item\n",
    "        item_text_chunks = text_splitter.split_text(item_text) # split the text into chunks\n",
    "        chunk_seq_id = 0\n",
    "        for chunk in item_text_chunks[:20]: # only take the first 20 chunks\n",
    "            form_id = file[file.rindex('/') + 1:file.rindex('.')] # extract form id from file name\n",
    "            # finally, construct a record with metadata and the chunk text\n",
    "            chunks_with_metadata.append({\n",
    "                'text': chunk, \n",
    "                # metadata from looping...\n",
    "                'item': item,\n",
    "                'chunkSeqId': chunk_seq_id,\n",
    "                # constructed metadata...\n",
    "                'formId': f'{form_id}', # pulled from the filename\n",
    "                'chunkId': f'{form_id}-{item}-chunk{chunk_seq_id:04d}',\n",
    "                # metadata from file...\n",
    "                'names': file_as_object['names'],\n",
    "                'cik': file_as_object['cik'],\n",
    "                'cusip6': file_as_object['cusip6'],\n",
    "                'source': file_as_object['source'],\n",
    "            })\n",
    "            chunk_seq_id += 1\n",
    "        print(f'\\tSplit into {chunk_seq_id} chunks')\n",
    "    return chunks_with_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a graph from the chunks\n",
    "\n",
    "You now have chunks prepared for creating a knowledge graph.\n",
    "\n",
    "The graph will have 1 node per chunk, containing the chunk text and metadata as properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge chunk query\n",
    "\n",
    "You will use a Cypher query to merge the chunks into the graph.\n",
    "\n",
    "This query accepts a query parameter called `chunkParam` which is expected\n",
    "to have the data record containing the chunk and metadata.\n",
    "\n",
    "The `MERGE` query will first match an existing node with the same `chunkId` property.\n",
    "\n",
    "If no such node exists, it will create a new node and the `ON CREATE` clause will set the properties using values from the `chunkParam` query parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_chunk_node_query = \"\"\"\n",
    "MERGE(mergedChunk:Chunk {chunkId: $chunkParam.chunkId})\n",
    "    ON CREATE SET \n",
    "        mergedChunk.names = $chunkParam.names,\n",
    "        mergedChunk.formId = $chunkParam.formId, \n",
    "        mergedChunk.cik = $chunkParam.cik, \n",
    "        mergedChunk.cusip6 = $chunkParam.cusip6, \n",
    "        mergedChunk.source = $chunkParam.source, \n",
    "        mergedChunk.item = $chunkParam.item, \n",
    "        mergedChunk.chunkSeqId = $chunkParam.chunkSeqId, \n",
    "        mergedChunk.text = $chunkParam.text\n",
    "RETURN mergedChunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create nodes for all chunks.\n",
    "# This will use the `merge_chunk_node_query` to create a `:Chunk` node for each chunk.\n",
    "def create_nodes_for_all_chunks(chunks_with_metadata_list):\n",
    "    node_count = 0\n",
    "    for chunk in chunks_with_metadata_list:\n",
    "        gdb.execute_query(merge_chunk_node_query, \n",
    "                chunkParam = chunk\n",
    "        )\n",
    "        node_count += 1\n",
    "    print(f\"Created {node_count} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare unique constraint\n",
    "\n",
    "Before calling the helper function to create a knowledge graph,\n",
    "we will take one extra step to make sure we don't duplicate data.\n",
    "\n",
    "The uniqueness constraint is also index. It's job is to ensure that\n",
    "a particular property is unique for all nodes that share a common label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a uniqueness constraint on the chunkId property of Chunk nodes \n",
    "gdb.execute_query(\"\"\"\n",
    "CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n",
    "    FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
    "\"\"\")\n",
    "\n",
    "created_indexes = gdb.execute_query('SHOW CONSTRAINTS').records\n",
    "print(created_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all form10 files\n",
    "\n",
    "Perform the node creation for all files in an import directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import glob\n",
    "\n",
    "IMPORT_DATA_DIRECTORY = f\"{DATA_DIR}/form10k/\"\n",
    "\n",
    "all_file_names = glob.glob(IMPORT_DATA_DIRECTORY + \"*.json\")\n",
    "counter = 0\n",
    "\n",
    "for file_name in all_file_names:\n",
    "    counter += 1\n",
    "    print(f'=== Processing {counter} of {len(all_file_names)} ===')\n",
    "    # get and split text data\n",
    "    print(f'Reading and splitting Form10k file {file_name}...')\n",
    "    chunk_list = split_form10k_data_from_file(file_name)\n",
    "    #load nodes\n",
    "    print('Creating Chunk Nodes...')\n",
    "    create_nodes_for_all_chunks(chunk_list)\n",
    "    print(f'Done Processing {file_name}')\n",
    "\n",
    "# Check the number of nodes in the graph\n",
    "gdb.execute_query(\"MATCH (n:Chunk) RETURN count(n) as chunkCount\").records[0].get('chunkCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique form IDs in the graph\n",
    "gdb.execute_query(\"MATCH (c:Chunk) RETURN count(distinct(c.formId)) as uniqueFormCount\").records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique company CUSIPs (company IDs) in the graph\n",
    "# Expect this to match the `uniqueCompanyCount` from the previous cell\n",
    "gdb.execute_query(\"MATCH (c:Chunk) RETURN count(distinct(c.cusip6)) as uniqueCompanyCount\").records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See a list of the company names in the graph\n",
    "gdb.execute_query(\"MATCH (c:Chunk) RETURN DISTINCT c.names\").records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhance - vector embeddings for the text of each chunk  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will use the `embeddings_api` defined in `shared.ipynb` to get the vector embeddings \n",
    "for the text of each chunk. This api will use an LLM to calculate an embedding for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple example of how to use the embeddings API\n",
    "text_embedding = embeddings_api.embed_query(\"embed this text using an LLM\")\n",
    "\n",
    "print(text_embedding)\n",
    "\n",
    "# all embeddings will have the same size, which is the dimensions of the vector\n",
    "vector_dimensions = len(text_embedding) \n",
    "\n",
    "print(f\"Text embeddings will have {vector_dimensions} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a vector index\n",
    "\n",
    "Now that you have a graph populated with `Chunk` nodes, \n",
    "you can add vector embeddings.\n",
    "\n",
    "First, prepare a vector index to store the embeddings.\n",
    "\n",
    "The index will be called `form_10k_chunks` and will store\n",
    "embeddings for nodes labeled as `Chunk` in a property\n",
    "called `textEmbedding`.\n",
    "\n",
    "The embeddings index will match the dimensions of the \n",
    "embeddings returned by the `embeddings_api` and will use \n",
    "the cosine similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector index called \"form_10k_chunks\" the `textEmbedding`` property of nodes labeled `Chunk`. \n",
    "# neo4j_create_vector_index(kg, VECTOR_INDEX_NAME, 'Chunk', 'textEmbedding')\n",
    "gdb.execute_query(\"\"\"\n",
    "         CREATE VECTOR INDEX `form_10k_chunks` IF NOT EXISTS\n",
    "          FOR (c:Chunk) ON (c.textEmbedding) \n",
    "          OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: $vectorDimensionsParam,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "         }}\n",
    "\"\"\",\n",
    "  vectorDimensionsParam = vector_dimensions\n",
    ")\n",
    "\n",
    "# Check the vector indexes in the graph\n",
    "gdb.execute_query('SHOW VECTOR INDEXES').records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text embeddings\n",
    "\n",
    "Creating the text embeddings will be a two step process. \n",
    "\n",
    "First, collect all chunk text and chunk ids from the graph.\n",
    "Yes these are the same chunk ids that were used to create the graph\n",
    "and you could save time by doing this all at once. We're doing\n",
    "this incrementally to show the process, not optimized for speed.\n",
    "\n",
    "Next, use the `embeddings_api` to get the embeddings for the text\n",
    "and write those values back into the graph. \n",
    "\n",
    "This will take some time to run as we're doing it one chunk at a time,\n",
    "calling out to the `embeddings_api` for each then writing all those\n",
    "results back into the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector embeddings for all the Chunk text, in batches.\n",
    "# Use this for larger number of chunks so that the query\n",
    "# can be re-run without losing all progress\n",
    "print(\"Finding all chunks that need textEmbedding...\")\n",
    "all_chunk_text_id = gdb.execute_query(\"\"\"\n",
    "  MATCH (chunk:Chunk) WHERE chunk.textEmbedding IS NULL\n",
    "  RETURN chunk.text AS text, chunk.chunkId AS chunkId\n",
    "  \"\"\").records\n",
    "\n",
    "print(\"Generating vector embeddings, then writing into each chunk...\")\n",
    "for chunk_text_id in all_chunk_text_id:\n",
    "  text_embedding = embeddings_api.embed_query(chunk_text_id['text'])\n",
    "  gdb.execute_query(\"\"\"\n",
    "    MATCH (chunk:Chunk {chunkId: $chunkIdParam})\n",
    "    CALL db.create.setNodeVectorProperty(chunk, \"textEmbedding\", $textEmbeddingParam)    \n",
    "    \"\"\", \n",
    "    chunkIdParam=chunk_text_id['chunkId'], textEmbeddingParam=text_embedding\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand - connect the chunks into linked lists\n",
    "\n",
    "You can now create relationships between all\n",
    "nodes in that list of chunks,\n",
    "effectively creating a linked list from the\n",
    "first chunk to the last.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the form IDs and form 10k item names\n",
    "distinct_form_id_result = gdb.execute_query(\"\"\"\n",
    "MATCH (c:Chunk) RETURN DISTINCT c.formId as formId\n",
    "\"\"\").records\n",
    "\n",
    "distinct_form_id_list = list(map(lambda x: x['formId'], distinct_form_id_result))\n",
    "\n",
    "# Connect *all* section chunks into a linked list..\n",
    "cypher = \"\"\"\n",
    "  MATCH (from_same_form_and_section:Chunk) // match all chunks\n",
    "  WHERE from_same_form_and_section.formId = $formIdParam // where the chunks are from the same form\n",
    "    AND from_same_form_and_section.item = $itemParam // and from the same section\n",
    "  WITH from_same_form_and_section // with those collections of chunks\n",
    "    ORDER BY from_same_form_and_section.chunkSeqId ASC // order the chunks by their sequence ID\n",
    "  WITH collect(from_same_form_and_section) as section_chunk_list // collect the chunks into a list\n",
    "    CALL apoc.nodes.link(section_chunk_list, \"NEXT\", {avoidDuplicates: true}) // then create a linked list in the graph\n",
    "  RETURN size(section_chunk_list)\n",
    "\"\"\"\n",
    "\n",
    "for form_id in distinct_form_id_list:\n",
    "  for form10kItemName in ['item1', 'item1a', 'item7', 'item7a']:\n",
    "    gdb.execute_query(cypher, \n",
    "             formIdParam=form_id, itemParam=form10kItemName\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example questions - vector similarity search with Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Neo4j vector search helper\n",
    "\n",
    "The `shared.ipynb` notebook has a helper function to perform a vector similarity search\n",
    "using the Neo4j Knowledge Graph.\n",
    "\n",
    "It will perform vector similarity search using the `form_10k_chunks` vector index.\n",
    "\n",
    "Try it out by searching for information about one of the companies in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = neo4j_vector_search(\n",
    "    'In a single sentence, tell me about Netapp.'\n",
    ")\n",
    "search_results[0].get('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering chat with Langchain \n",
    "\n",
    "Notice that we only performed vector search. So what we're getting\n",
    "back is the raw chunk text.\n",
    "\n",
    "If we want to create a chatbot that provides actual answers to\n",
    "a question, we can build a RAG system using Langchain.\n",
    "\n",
    "The basic RAG flow goes through these steps:\n",
    "\n",
    "1. accept a question from the user\n",
    "2. perform a database query to find relevant text that may provide an answer\n",
    "3. package the original question plus the relevant text into a prompt\n",
    "4. pass the entire prompt to an LLM to produce an answer\n",
    "5. finally, return the LLM's answer to the user\n",
    "\n",
    "Langchain is a great framework for creating a complete RAG workflow.\n",
    "\n",
    "It has excellent integration with Neo4j. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the chat api directly\n",
    "result = chat_api.invoke(\"What is the capital of France?\")\n",
    "\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neo4j Vector Store\n",
    "\n",
    "The easiest way to start using Neo4j with Langchain\n",
    "is with the `Neo4jVector` interface.\n",
    "\n",
    "This makes Neo4j look like a vector store using\n",
    "the vector index you created earlier.\n",
    "\n",
    "Under the hood, it will use the Cypher language\n",
    "for performing vector similarity searches.\n",
    "\n",
    "The configuration specifies a few important things:\n",
    "- use the defined `embeddings_api` for embeddings\n",
    "- how to connect to the Neo4j database\n",
    "- the name of the vector index to use\n",
    "- the label of the nodes to search\n",
    "- the property name of the text on those nodes\n",
    "- and, the property name of the embeddings on those nodes\n",
    "\n",
    "That vector store then gets converted into a retriever\n",
    "and finally added to a Question Answering chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a langchain vector store from the existing Neo4j knowledge graph.\n",
    "neo4j_vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings_api,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    node_label=VECTOR_NODE_LABEL,\n",
    "    text_node_properties=[VECTOR_SOURCE_PROPERTY],\n",
    "    embedding_node_property=VECTOR_EMBEDDING_PROPERTY,\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = neo4j_vector_store.as_retriever()\n",
    "\n",
    "# Create a chatbot Question & Answer chain from the retriever\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    chat_api, chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "\n",
    "prettyVectorSearch = prettifyChain(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask some questions\n",
    "\n",
    "Finally, you can use the Langchain chain, which combines the retriever\n",
    "and the vector store into a nice question and answer interface.\n",
    "\n",
    "You can see both the answer and the source that the answer came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyVectorSearch(\"What is Netapp's primary business?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyVectorSearch(\"Where is Netapp headquartered?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyVectorSearch(\"Briefly tell me about Netapp in a single sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
